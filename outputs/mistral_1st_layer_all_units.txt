0.0
loading model
smoothing
tokenizer
loading dataset lambada
loading dataset wikitext
quantizing...
layers.0.self_attn
noisy_model_quantized
MistralForCausalLM(
  (model): MistralModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): NoisyW8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.0)
          (k_proj): NoisyW8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.0)
          (v_proj): NoisyW8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.0)
          (o_proj): NoisyW8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None, err_prob=0.0)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): NoisyW8A8MatMul(act_quant=per_token, output_quant=None, err_prob=0.0)
          (matmul2): NoisyW8A8MatMul(act_quant=per_token, output_quant=per_token, err_prob=0.0)
        )
        (mlp): MistralMLP(
          (gate_proj): NoisyW8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.0)
          (up_proj): NoisyW8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.0)
          (down_proj): NoisyW8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.0)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (1): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (2): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (3): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (4): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (5): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (6): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (7): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (8): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (9): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (10): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (11): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (12): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (13): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (14): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (15): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (16): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (17): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (18): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (19): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (20): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (21): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (22): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (23): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (24): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (25): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (26): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (27): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (28): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (29): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (30): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (31): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
    )
    (norm): MistralRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
evaluating
87.061
5.634078502655029
time_i 28.372180151939393
1e-08
loading model
smoothing
tokenizer
loading dataset lambada
loading dataset wikitext
quantizing...
layers.0.self_attn
noisy_model_quantized
MistralForCausalLM(
  (model): MistralModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): NoisyW8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-08)
          (k_proj): NoisyW8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-08)
          (v_proj): NoisyW8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-08)
          (o_proj): NoisyW8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None, err_prob=1e-08)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): NoisyW8A8MatMul(act_quant=per_token, output_quant=None, err_prob=1e-08)
          (matmul2): NoisyW8A8MatMul(act_quant=per_token, output_quant=per_token, err_prob=1e-08)
        )
        (mlp): MistralMLP(
          (gate_proj): NoisyW8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-08)
          (up_proj): NoisyW8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-08)
          (down_proj): NoisyW8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-08)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (1): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (2): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (3): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (4): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (5): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (6): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (7): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (8): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (9): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (10): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (11): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (12): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (13): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (14): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (15): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (16): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (17): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (18): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (19): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (20): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (21): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (22): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (23): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (24): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (25): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (26): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (27): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (28): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (29): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (30): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (31): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
    )
    (norm): MistralRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
evaluating
87.04
5.641757965087891
time_i 28.03786946137746
1e-07
loading model
smoothing
tokenizer
loading dataset lambada
loading dataset wikitext
quantizing...
layers.0.self_attn
noisy_model_quantized
MistralForCausalLM(
  (model): MistralModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): NoisyW8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-07)
          (k_proj): NoisyW8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-07)
          (v_proj): NoisyW8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-07)
          (o_proj): NoisyW8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None, err_prob=1e-07)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): NoisyW8A8MatMul(act_quant=per_token, output_quant=None, err_prob=1e-07)
          (matmul2): NoisyW8A8MatMul(act_quant=per_token, output_quant=per_token, err_prob=1e-07)
        )
        (mlp): MistralMLP(
          (gate_proj): NoisyW8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-07)
          (up_proj): NoisyW8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-07)
          (down_proj): NoisyW8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-07)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (1): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (2): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (3): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (4): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (5): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (6): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (7): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (8): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (9): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (10): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (11): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (12): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (13): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (14): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (15): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (16): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (17): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (18): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (19): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (20): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (21): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (22): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (23): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (24): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (25): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (26): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (27): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (28): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (29): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (30): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (31): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
    )
    (norm): MistralRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
evaluating
86.773
5.660421848297119
time_i 28.280798129240672
1e-06
loading model
smoothing
tokenizer
loading dataset lambada
loading dataset wikitext
quantizing...
layers.0.self_attn
noisy_model_quantized
MistralForCausalLM(
  (model): MistralModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): NoisyW8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-06)
          (k_proj): NoisyW8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-06)
          (v_proj): NoisyW8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-06)
          (o_proj): NoisyW8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None, err_prob=1e-06)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): NoisyW8A8MatMul(act_quant=per_token, output_quant=None, err_prob=1e-06)
          (matmul2): NoisyW8A8MatMul(act_quant=per_token, output_quant=per_token, err_prob=1e-06)
        )
        (mlp): MistralMLP(
          (gate_proj): NoisyW8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-06)
          (up_proj): NoisyW8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-06)
          (down_proj): NoisyW8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-06)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (1): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (2): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (3): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (4): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (5): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (6): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (7): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (8): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (9): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (10): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (11): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (12): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (13): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (14): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (15): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (16): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (17): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (18): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (19): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (20): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (21): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (22): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (23): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (24): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (25): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (26): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (27): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (28): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (29): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (30): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (31): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
    )
    (norm): MistralRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
evaluating
85.623
6.172791957855225
time_i 28.050950376192727
1e-05
loading model
smoothing
tokenizer
loading dataset lambada
loading dataset wikitext
quantizing...
layers.0.self_attn
noisy_model_quantized
MistralForCausalLM(
  (model): MistralModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): NoisyW8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-05)
          (k_proj): NoisyW8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-05)
          (v_proj): NoisyW8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-05)
          (o_proj): NoisyW8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None, err_prob=1e-05)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): NoisyW8A8MatMul(act_quant=per_token, output_quant=None, err_prob=1e-05)
          (matmul2): NoisyW8A8MatMul(act_quant=per_token, output_quant=per_token, err_prob=1e-05)
        )
        (mlp): MistralMLP(
          (gate_proj): NoisyW8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-05)
          (up_proj): NoisyW8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-05)
          (down_proj): NoisyW8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=1e-05)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (1): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (2): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (3): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (4): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (5): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (6): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (7): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (8): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (9): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (10): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (11): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (12): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (13): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (14): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (15): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (16): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (17): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (18): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (19): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (20): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (21): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (22): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (23): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (24): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (25): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (26): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (27): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (28): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (29): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (30): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (31): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
    )
    (norm): MistralRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
evaluating
74.081
21.812252044677734
time_i 27.993521900971732
0.0001
loading model
smoothing
tokenizer
loading dataset lambada
loading dataset wikitext
quantizing...
layers.0.self_attn
noisy_model_quantized
MistralForCausalLM(
  (model): MistralModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): NoisyW8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.0001)
          (k_proj): NoisyW8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.0001)
          (v_proj): NoisyW8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.0001)
          (o_proj): NoisyW8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None, err_prob=0.0001)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): NoisyW8A8MatMul(act_quant=per_token, output_quant=None, err_prob=0.0001)
          (matmul2): NoisyW8A8MatMul(act_quant=per_token, output_quant=per_token, err_prob=0.0001)
        )
        (mlp): MistralMLP(
          (gate_proj): NoisyW8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.0001)
          (up_proj): NoisyW8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.0001)
          (down_proj): NoisyW8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.0001)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (1): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (2): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (3): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (4): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (5): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (6): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (7): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (8): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (9): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (10): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (11): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (12): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (13): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (14): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (15): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (16): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (17): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (18): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (19): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (20): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (21): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (22): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (23): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (24): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (25): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (26): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (27): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (28): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (29): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (30): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (31): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
    )
    (norm): MistralRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
evaluating
18.546
2217.41796875
time_i 28.000861978530885
0.001
loading model
smoothing
tokenizer
loading dataset lambada
loading dataset wikitext
quantizing...
layers.0.self_attn
noisy_model_quantized
MistralForCausalLM(
  (model): MistralModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): NoisyW8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.001)
          (k_proj): NoisyW8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.001)
          (v_proj): NoisyW8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.001)
          (o_proj): NoisyW8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None, err_prob=0.001)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): NoisyW8A8MatMul(act_quant=per_token, output_quant=None, err_prob=0.001)
          (matmul2): NoisyW8A8MatMul(act_quant=per_token, output_quant=per_token, err_prob=0.001)
        )
        (mlp): MistralMLP(
          (gate_proj): NoisyW8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.001)
          (up_proj): NoisyW8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.001)
          (down_proj): NoisyW8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.001)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (1): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (2): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (3): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (4): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (5): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (6): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (7): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (8): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (9): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (10): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (11): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (12): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (13): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (14): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (15): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (16): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (17): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (18): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (19): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (20): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (21): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (22): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (23): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (24): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (25): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (26): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (27): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (28): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (29): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (30): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (31): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
    )
    (norm): MistralRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
evaluating
0.041
65862.3125
time_i 28.07999858458837
0.01
loading model
smoothing
tokenizer
loading dataset lambada
loading dataset wikitext
quantizing...
layers.0.self_attn
noisy_model_quantized
MistralForCausalLM(
  (model): MistralModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): NoisyW8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.01)
          (k_proj): NoisyW8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.01)
          (v_proj): NoisyW8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.01)
          (o_proj): NoisyW8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None, err_prob=0.01)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): NoisyW8A8MatMul(act_quant=per_token, output_quant=None, err_prob=0.01)
          (matmul2): NoisyW8A8MatMul(act_quant=per_token, output_quant=per_token, err_prob=0.01)
        )
        (mlp): MistralMLP(
          (gate_proj): NoisyW8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.01)
          (up_proj): NoisyW8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.01)
          (down_proj): NoisyW8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token, err_prob=0.01)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (1): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (2): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (3): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (4): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (5): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (6): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (7): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (8): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (9): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (10): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (11): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (12): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (13): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (14): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (15): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (16): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (17): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (18): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (19): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (20): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (21): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (22): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (23): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (24): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (25): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (26): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (27): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (28): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (29): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (30): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
      (31): MistralDecoderLayer(
        (self_attn): MistralAttention(
          (q_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (k_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (v_proj): W8A8Linear(4096, 1024, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (o_proj): W8A8Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)
          (rotary_emb): MistralRotaryEmbedding()
          (matmul1): W8A8MatMul(act_quant=per_token, output_quant=None)
          (matmul2): W8A8MatMul(act_quant=per_token, output_quant=per_token)
        )
        (mlp): MistralMLP(
          (gate_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (up_proj): W8A8Linear(4096, 14336, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (down_proj): W8A8Linear(14336, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=per_token)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
    )
    (norm): MistralRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
evaluating
0.0
79866.4140625
time_i 28.15870407025019
acc_noisy_list [87.061, 87.04, 86.773, 85.623, 74.081, 18.546, 0.041, 0.0]
87.061
87.04
86.773
85.623
74.081
18.546
0.041
0.0
ppl_noisy_list [5.634078502655029, 5.641757965087891, 5.660421848297119, 6.172791957855225, 21.812252044677734, 2217.41796875, 65862.3125, 79866.4140625]
5.634078502655029
5.641757965087891
5.660421848297119
6.172791957855225
21.812252044677734
2217.41796875
65862.3125
79866.4140625
x_sum_list []
time_sum, 225.00042624870937
